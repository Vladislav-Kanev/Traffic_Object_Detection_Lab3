{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseTrainPath = 'data/train_images/train_images/'\n",
    "\n",
    "train_file = open('data/usdc_train.json')\n",
    "\n",
    "data_json = json.load(train_file)\n",
    " \n",
    "classes = data_json['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data_json['annotations'])\n",
    "images = dict({i['id']: i['file_name'] for i in data_json['images']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>area</th>\n",
       "      <th>segmentation</th>\n",
       "      <th>iscrowd</th>\n",
       "      <th>confidence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[196, 234, 20.266666666666666, 24.74666666666667]</td>\n",
       "      <td>501.532444</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[247, 236, 16.53333333333333, 24.74666666666667]</td>\n",
       "      <td>409.144889</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[265, 234, 20.8, 23.893333333333334]</td>\n",
       "      <td>496.981333</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[267, 237, 15.466666666666667, 17.066666666666...</td>\n",
       "      <td>263.964444</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[305, 233, 78.93333333333334, 75.94666666666667]</td>\n",
       "      <td>5994.723556</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  image_id  category_id  \\\n",
       "0  20         5            2   \n",
       "1  21         5            2   \n",
       "2  22         5            2   \n",
       "3  23         5            2   \n",
       "4  24         5            2   \n",
       "\n",
       "                                                bbox         area  \\\n",
       "0  [196, 234, 20.266666666666666, 24.74666666666667]   501.532444   \n",
       "1   [247, 236, 16.53333333333333, 24.74666666666667]   409.144889   \n",
       "2               [265, 234, 20.8, 23.893333333333334]   496.981333   \n",
       "3  [267, 237, 15.466666666666667, 17.066666666666...   263.964444   \n",
       "4   [305, 233, 78.93333333333334, 75.94666666666667]  5994.723556   \n",
       "\n",
       "  segmentation  iscrowd  confidence  score  \n",
       "0           []        0         0.5    0.5  \n",
       "1           []        0         0.5    0.5  \n",
       "2           []        0         0.5    0.5  \n",
       "3           []        0         0.5    0.5  \n",
       "4           []        0         0.5    0.5  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>[[196, 234, 216.26666666666665, 258.7466666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>[2, 8, 1, 2, 2, 2, 8, 7, 7, 2, 7, 2]</td>\n",
       "      <td>[[0, 224, 22.666666666666668, 264.533333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>[11, 2, 2, 2]</td>\n",
       "      <td>[[3, 230, 67, 299.97333333333336], [82, 241, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[[228, 261, 244.26666666666665, 284.04], [198,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[[72, 268, 117.33333333333334, 311.52], [157, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10592</th>\n",
       "      <td>29792</td>\n",
       "      <td>[2, 2, 2, 2, 11, 3, 3]</td>\n",
       "      <td>[[89, 230, 155.13333333333333, 317.89333333333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10593</th>\n",
       "      <td>29793</td>\n",
       "      <td>[2, 2, 2, 2]</td>\n",
       "      <td>[[65, 269, 112.46666666666667, 311.66666666666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10594</th>\n",
       "      <td>29795</td>\n",
       "      <td>[5, 5, 3, 3, 2]</td>\n",
       "      <td>[[193, 137, 203.66666666666666, 180.52], [270,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10595</th>\n",
       "      <td>29796</td>\n",
       "      <td>[2, 7, 7]</td>\n",
       "      <td>[[235, 256, 250.73333333333332, 266.24], [239,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10596</th>\n",
       "      <td>29798</td>\n",
       "      <td>[2, 2, 2, 7, 7, 3]</td>\n",
       "      <td>[[0, 210, 80, 384.93333333333334], [64, 219, 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10597 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                           category_id  \\\n",
       "0             5                       [2, 2, 2, 2, 2]   \n",
       "1            14  [2, 8, 1, 2, 2, 2, 8, 7, 7, 2, 7, 2]   \n",
       "2            22                         [11, 2, 2, 2]   \n",
       "3            58                                [2, 2]   \n",
       "4            74                                [2, 2]   \n",
       "...         ...                                   ...   \n",
       "10592     29792                [2, 2, 2, 2, 11, 3, 3]   \n",
       "10593     29793                          [2, 2, 2, 2]   \n",
       "10594     29795                       [5, 5, 3, 3, 2]   \n",
       "10595     29796                             [2, 7, 7]   \n",
       "10596     29798                    [2, 2, 2, 7, 7, 3]   \n",
       "\n",
       "                                                    bbox  \n",
       "0      [[196, 234, 216.26666666666665, 258.7466666666...  \n",
       "1      [[0, 224, 22.666666666666668, 264.533333333333...  \n",
       "2      [[3, 230, 67, 299.97333333333336], [82, 241, 1...  \n",
       "3      [[228, 261, 244.26666666666665, 284.04], [198,...  \n",
       "4      [[72, 268, 117.33333333333334, 311.52], [157, ...  \n",
       "...                                                  ...  \n",
       "10592  [[89, 230, 155.13333333333333, 317.89333333333...  \n",
       "10593  [[65, 269, 112.46666666666667, 311.66666666666...  \n",
       "10594  [[193, 137, 203.66666666666666, 180.52], [270,...  \n",
       "10595  [[235, 256, 250.73333333333332, 266.24], [239,...  \n",
       "10596  [[0, 210, 80, 384.93333333333334], [64, 219, 9...  \n",
       "\n",
       "[10597 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.DataFrame(data)\n",
    "ds['bbox'] = [[i[0], i[1], i[0] + i[2], i[1] + i[3]] for i in data.bbox]\n",
    "ds = ds.groupby('image_id').agg({'category_id': list, 'bbox': list}).reset_index()\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['file_name'] = [images[i] for i in ds.image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>[2, 2, 2, 2, 2]</td>\n",
       "      <td>[[196, 234, 216.26666666666665, 258.7466666666...</td>\n",
       "      <td>1478020898717725646_jpg.rf.68EjFVQdDWrB0SW6qVl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>[2, 8, 1, 2, 2, 2, 8, 7, 7, 2, 7, 2]</td>\n",
       "      <td>[[0, 224, 22.666666666666668, 264.533333333333...</td>\n",
       "      <td>1478020650710690845_jpg.rf.68NUjFyrbU9Nsyt3ika...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>[11, 2, 2, 2]</td>\n",
       "      <td>[[3, 230, 67, 299.97333333333336], [82, 241, 1...</td>\n",
       "      <td>1478898975867837103_jpg.rf.6847BekYxQ4SlhtvjlE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[[228, 261, 244.26666666666665, 284.04], [198,...</td>\n",
       "      <td>1478896447910420804_jpg.rf.68xeyVz6sbVFpLVku7W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[[72, 268, 117.33333333333334, 311.52], [157, ...</td>\n",
       "      <td>1478900272123772897_jpg.rf.694dEtyhq6YM051Zxsj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                           category_id  \\\n",
       "0         5                       [2, 2, 2, 2, 2]   \n",
       "1        14  [2, 8, 1, 2, 2, 2, 8, 7, 7, 2, 7, 2]   \n",
       "2        22                         [11, 2, 2, 2]   \n",
       "3        58                                [2, 2]   \n",
       "4        74                                [2, 2]   \n",
       "\n",
       "                                                bbox  \\\n",
       "0  [[196, 234, 216.26666666666665, 258.7466666666...   \n",
       "1  [[0, 224, 22.666666666666668, 264.533333333333...   \n",
       "2  [[3, 230, 67, 299.97333333333336], [82, 241, 1...   \n",
       "3  [[228, 261, 244.26666666666665, 284.04], [198,...   \n",
       "4  [[72, 268, 117.33333333333334, 311.52], [157, ...   \n",
       "\n",
       "                                           file_name  \n",
       "0  1478020898717725646_jpg.rf.68EjFVQdDWrB0SW6qVl...  \n",
       "1  1478020650710690845_jpg.rf.68NUjFyrbU9Nsyt3ika...  \n",
       "2  1478898975867837103_jpg.rf.6847BekYxQ4SlhtvjlE...  \n",
       "3  1478896447910420804_jpg.rf.68xeyVz6sbVFpLVku7W...  \n",
       "4  1478900272123772897_jpg.rf.694dEtyhq6YM051Zxsj...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, imageBasePath, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "        self.imageBasePath = imageBasePath\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.dataset['file_name'][idx]\n",
    "        image = Image.open(self.imageBasePath + image_name).convert('RGB')\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = torch.as_tensor(self.dataset['bbox'][idx])\n",
    "        target['labels'] = torch.as_tensor(self.dataset['category_id'][idx])\n",
    "        \n",
    "        return T.ToTensor()(image), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds, test_inds = train_test_split(range(len(ds)), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(CustomDataset(ds, baseTrainPath),\n",
    "                                       batch_size=16,\n",
    "                                       shuffle=True,\n",
    "                                       collate_fn = custom_collate,\n",
    "                                       pin_memory = True if torch.cuda.is_available() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladislav/projects/ml/lab3/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vladislav/projects/ml/lab3/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = len(classes)\n",
    "in_featured = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_featured, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vladislav/projects/ml/lab3/test.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/vladislav/projects/ml/lab3/test.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     targ[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m d[\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/vladislav/projects/ml/lab3/test.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     targets\u001b[39m.\u001b[39mappend(targ)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/vladislav/projects/ml/lab3/test.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m loss_dict \u001b[39m=\u001b[39m model(imgs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/vladislav/projects/ml/lab3/test.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/vladislav/projects/ml/lab3/test.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn(images, features, targets)\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:372\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    370\u001b[0m proposals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_coder\u001b[39m.\u001b[39mdecode(pred_bbox_deltas\u001b[39m.\u001b[39mdetach(), anchors)\n\u001b[1;32m    371\u001b[0m proposals \u001b[39m=\u001b[39m proposals\u001b[39m.\u001b[39mview(num_images, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m--> 372\u001b[0m boxes, scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_proposals(proposals, objectness, images\u001b[39m.\u001b[39;49mimage_sizes, num_anchors_per_level)\n\u001b[1;32m    374\u001b[0m losses \u001b[39m=\u001b[39m {}\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:262\u001b[0m, in \u001b[0;36mRegionProposalNetwork.filter_proposals\u001b[0;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[1;32m    259\u001b[0m levels \u001b[39m=\u001b[39m levels\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand_as(objectness)\n\u001b[1;32m    261\u001b[0m \u001b[39m# select top_n boxes independently per level before applying nms\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m top_n_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_top_n_idx(objectness, num_anchors_per_level)\n\u001b[1;32m    264\u001b[0m image_range \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(num_images, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    265\u001b[0m batch_idx \u001b[39m=\u001b[39m image_range[:, \u001b[39mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/projects/ml/lab3/venv/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:236\u001b[0m, in \u001b[0;36mRegionProposalNetwork._get_top_n_idx\u001b[0;34m(self, objectness, num_anchors_per_level)\u001b[0m\n\u001b[1;32m    234\u001b[0m num_anchors \u001b[39m=\u001b[39m ob\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    235\u001b[0m pre_nms_top_n \u001b[39m=\u001b[39m det_utils\u001b[39m.\u001b[39m_topk_min(ob, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_nms_top_n(), \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m _, top_n_idx \u001b[39m=\u001b[39m ob\u001b[39m.\u001b[39;49mtopk(pre_nms_top_n, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    237\u001b[0m r\u001b[39m.\u001b[39mappend(top_n_idx \u001b[39m+\u001b[39m offset)\n\u001b[1;32m    238\u001b[0m offset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_anchors\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for data in train_dl:\n",
    "        imgs = []\n",
    "        targets = []\n",
    "        for d in data:\n",
    "            imgs.append(d[0].to(device))\n",
    "            targ = {}\n",
    "            targ['boxes'] = d[1]['boxes'].to(device)\n",
    "            targ['labels'] = d[1]['labels'].to(device)\n",
    "            targets.append(targ)\n",
    "        loss_dict = model(imgs, targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
